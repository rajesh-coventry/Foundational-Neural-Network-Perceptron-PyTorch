{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnxZerJKi2T11m84kDw3Z9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajesh-coventry/Foundational-Neural-Network-Perceptron-PyTorch/blob/master/01_Foundational_Neural_Network_(Perceptron).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Foundational Neural Network (Perceptron):**"
      ],
      "metadata": {
        "id": "LNdeX2kGrkBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron is a commonly used term in the arena of `Machine Learning` and `Artificial Intelligence`. Being the most basic component of Machine Learning and Deep Learning technologies, the perceptron is the elementary unit of an `Artificial Neural Network`."
      ],
      "metadata": {
        "id": "_W6ZfT67s_ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Perceptron?**\n",
        "\n",
        "- A perceptron is the `smallest element` of a neural network.\n",
        "\n",
        "- Perceptron is a `single-layer neural network` or a Machine Learning algorithm.\n",
        "\n",
        "- It works as an `artificial neuron` to perform computations by learning elements and processing them for detecting the business intelligence and capabilities of the input data.\n",
        "\n",
        "- A `perceptron network` is a group of simple logical statements that come together to create an array of complex logical statements, known as the `neural network`."
      ],
      "metadata": {
        "id": "pQEYfMqdtb8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ![](https://www.analytixlabs.co.in/wp-content/uploads/2022/07/biological-neuron-analytix-labs.jpg)"
      ],
      "metadata": {
        "id": "Ufl3eQHFuPFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The human brain is a `complex network` of billions of interconnected cells known as `Neurons`. These cells process and transmit signals. Biological neurons respond to both `chemical and electrical signals` to create the `Biological Neural Network` $(BNN)$.\n",
        "\n",
        "- The input and output signals can either be `excitatory` or `inhibitory`, meaning that they can either increase or decrease the potential of the neuron to fire.\n",
        "\n",
        "- The structure of a biological neuron consists of a `Synapse`, `dendrites`, `Soma` or `the cell body`, and `axon`. All these components participate in the `neural processing` performed by `neurons`. `Synapse` connects an `axon` to another `neuron` and also processes the inputs. `Dendrites` receive the signals while the `Soma` sums up all the incoming signals. The transmission of signals to other neurons is carried by the `axon`. A `Biological Neural Network` slowly yet efficiently processes highly complex parallel inputs."
      ],
      "metadata": {
        "id": "AxtswhwQuV7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Artificial Neuron:**\n",
        "\n",
        "- An `artificial neuron` is based on a model of `biological neurons` but it is a `mathematical function`.\n",
        "\n",
        "- The neuron takes inputs in the form of `binary values` i.e. `1` or `0`, meaning that they can either be `ON` or `OFF`.\n",
        "\n",
        "- The output of an `artificial neuron` is usually calculated by applying a `threshold function` to the sum of its `input values`.\n",
        "\n",
        "- The `threshold function` can be either `linear` or `nonlinear`. A `linear threshold function` produces an output of 1 if the sum of the input values is greater than or equal to a certain threshold, and an output of 0 if the sum of the input values is less than that threshold. A `nonlinear threshold function`, on the other hand, can produce any output value between 0 and 1, depending on the inputs.\n",
        "\n",
        "- An `Artificial Neural Network` $(ANN)$ is built on artificial neurons and based on a `Feed-Forward` strategy. It is known as the simplest type of neural network as it continues learning irrespective of the data being `linear` or `nonlinear`. The information flow through the nodes is continuous and stops only after reaching the output node."
      ],
      "metadata": {
        "id": "5koWrhO1vjtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Biological Neural Network Vs Artificial Neural Network:**\n",
        "\n",
        "The structure of `artificial neurons` is derived from `biological neurons` and the network is also formed on a similar principle but there are some differences between a `biological neural network` and an `artificial neural network`."
      ],
      "metadata": {
        "id": "rAWUD7EFvqoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ![](https://www.analytixlabs.co.in/wp-content/uploads/2022/07/Biological-Neural-Network-Vs-Artificial-Neural-Network.jpg)"
      ],
      "metadata": {
        "id": "2FlyFSF9vvF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perceptron Vs Neuron:**\n",
        "\n",
        "- The `perceptron` is a mathematical model of the `biological neuron`. It produces binary outputs from input values while taking into consideration `weights` and `threshold values`. Though created to imitate the working of `biological neurons`, the perceptron model has since been replaced by more advanced models like `backpropagation networks` for training `artificial neural networks`. Perceptrons use a brittle activation function to give a positive or negative output based on a specific value.\n",
        "\n",
        "- A `neuron`, also known as a node in a `backpropagation artificial neural network produces graded values between 0 and 1. It is a generalization of the idea of the perceptron as the neuron also adds weighted inputs. However, it does not produce a binary output but a graded value based on the proximity of the input to the desired value of 1. The results are biased towards the extreme values of 0 or 1 as the node uses a sigmoidal output function. The graded values can be interpreted to define the probability of the input’s category."
      ],
      "metadata": {
        "id": "I1CtWz802c-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Components of a Perceptron:**"
      ],
      "metadata": {
        "id": "TIGTlvRL2c7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ![](https://www.analytixlabs.co.in/wp-content/uploads/2022/07/04.jpg)"
      ],
      "metadata": {
        "id": "m3AKpbJS2c4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each perceptron comprises four different parts:**\n",
        "\n",
        "1. **`Input Values`:** A set of values or a dataset for predicting the output value. They are also described as a dataset’s features and dataset.\n",
        "\n",
        "2. **`Weights`:** The real value of each feature is known as weight. It tells the importance of that feature in predicting the final value.\n",
        "\n",
        "3. **`Bias`:** The activation function is shifted towards the left or right using bias. You may understand it simply as the y-intercept in the line equation.\n",
        "\n",
        "4. **`Summation Function`:** The summation function binds the weights and inputs together. It is a function to find their sum.\n",
        "\n",
        "5. **`Activation Function`:** It introduces non-linearity in the perceptron model."
      ],
      "metadata": {
        "id": "mGuUVitg2cyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why do we Need Weight and Bias?**\n",
        "\n",
        "- `Weight` and `bias` are two important aspects of the perceptron model.\n",
        "\n",
        "- These are `learnable parameters` and as the network gets trained it adjusts both parameters to achieve the desired values and the correct output."
      ],
      "metadata": {
        "id": "JSzvziB02cvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ![](https://www.analytixlabs.co.in/wp-content/uploads/2022/07/05.jpg)"
      ],
      "metadata": {
        "id": "9e3JGyK_2co3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Weights` are used to measure the `importance of each feature` in predicting output value.\n",
        "\n",
        "- Features with values close to zero are said to have lesser weight or significance. These have less importance in the prediction process compared to the features with values further from zero known as `weights with a larger value`.\n",
        "\n",
        "- Besides, high-weighted features having greater predictive power than low-weighting ones, the weight can also be positive or negative.\n",
        "\n",
        "- If the weight of a feature is positive then it has a direct relation with the target value, and if it is negative then it has an inverse relationship with the target value."
      ],
      "metadata": {
        "id": "nCOekIvY2cmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ![](https://www.analytixlabs.co.in/wp-content/uploads/2022/07/06.jpg)"
      ],
      "metadata": {
        "id": "5XejxT5p2cjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to `weight` in a neural network that increases the speed of triggering an activation function, `bias` delays the trigger of the `activation function`.\n",
        "\n",
        "It acts like an `intercept` in a linear equation. Simply stated, `Bias` is a constant used to adjust the output and help the model to provide the best fit output for the given data."
      ],
      "metadata": {
        "id": "Pa5Qsv6U2chC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "H_eGtXZkBmJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perceptron is a fundamental computational unit in machine learning and neural networks, representing one of the earliest and most important concepts in artificial intelligence.\n",
        "\n",
        "## **What is a Perceptron?**\n",
        "\n",
        "A perceptron is a linear binary classifier that takes multiple inputs, applies weights to them, sums them up, and produces a binary output (0 or 1, or -1 and +1) based on whether the weighted sum exceeds a certain threshold. It was invented by Frank Rosenblatt in 1957 and represents a mathematical model inspired by biological neurons.\n",
        "\n",
        "The basic structure consists of:\n",
        "- **`Input layer`**: Receives input features ($x₁$, $x₂$, ..., $xₙ$)\n",
        "\n",
        "- **`Weights`**: Each input has an associated weight ($w₁$, $w₂$, ..., $wₙ$)\n",
        "\n",
        "- **`Bias`**: An additional parameter $(b)$ that shifts the decision boundary\n",
        "\n",
        "- **`Activation function`**: Typically a step function that produces binary output\n",
        "\n",
        "- **`Output`**: A single binary classification result\n",
        "\n",
        "**Mathematical Representation:**\n",
        "\n",
        "**The perceptron's output is calculated as:**\n",
        "\n",
        "```\n",
        "y = f(∑(wᵢ × xᵢ) + b)\n",
        "```\n",
        "\n",
        "Where $f$ is the activation function, typically:\n",
        "\n",
        "```\n",
        "f(z) = 1 if z ≥ 0\n",
        "f(z) = 0 if z < 0\n",
        "```\n",
        "\n",
        "## **Is Perceptron the Most Basic Building Block?**\n",
        "\n",
        "Yes and no. The perceptron is historically the most basic form of an `artificial neuron` and serves as the conceptual foundation for neural networks. However, modern neural networks use more sophisticated units:\n",
        "\n",
        "**As a building block:**\n",
        "- The `perceptron` introduced the concept of `weighted inputs` and `thresholding`\n",
        "\n",
        "- It established the framework for `learning through weight adjustment`\n",
        "\n",
        "- Modern neurons in neural networks are essentially `enhanced perceptrons` with different activation functions\n",
        "\n",
        "**Limitations as a building block:**\n",
        "- Single perceptrons can only solve linearly separable problems\n",
        "\n",
        "- Modern neural networks use neurons with continuous activation functions (`sigmoid`, `ReLU`, `tanh`)\n",
        "\n",
        "- Deep networks require more sophisticated architectures than `simple perceptron stacking`\n",
        "\n",
        "## **Perceptron Learning Rules:**\n",
        "\n",
        "### **1. Perceptron Learning Rule (Original):**\n",
        "\n",
        "This is the most fundamental learning algorithm:"
      ],
      "metadata": {
        "id": "AeiEpCXW2bw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Algorithm:`**\n",
        "\n",
        "```\n",
        "For each training example (x, target):\n",
        "1. Calculate output: y = sign(w·x + b)\n",
        "2. Calculate error: error = target - y\n",
        "3. Update weights: w = w + α × error × x\n",
        "4. Update bias: b = b + α × error\n",
        "```\n",
        "\n",
        "Where $α$ $(alpha)$ is the learning rate.\n",
        "\n",
        "**Key characteristics:**\n",
        "- Guaranteed to converge for linearly separable data\n",
        "\n",
        "- May not converge for non-linearly separable data\n",
        "\n",
        "- Updates weights only when classification is incorrect\n",
        "\n",
        "### **2. Delta Rule (Widrow-Hoff Rule):**\n",
        "\n",
        "An improved version that uses continuous error:\n",
        "\n",
        "**`Algorithm:`**\n",
        "\n",
        "```\n",
        "For each training example (x, target):\n",
        "1. Calculate net input: net = w·x + b\n",
        "2. Calculate error: error = target - net\n",
        "3. Update weights: w = w + α × error × x\n",
        "4. Update bias: b = b + α × error\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- Uses continuous error rather than binary error\n",
        "\n",
        "- More stable convergence properties\n",
        "\n",
        "- Forms the basis for backpropagation in neural networks\n",
        "\n",
        "### **3. Pocket Algorithm:**\n",
        "\n",
        "**Designed for non-linearly separable data:**\n",
        "\n",
        "**Algorithm:**\n",
        "- Maintains the best weight vector found so far\n",
        "\n",
        "- Updates weights using perceptron rule\n",
        "\n",
        "- Keeps track of the longest run of correct classifications\n",
        "\n",
        "- `\"Pockets\"` the best weights when a better solution is found"
      ],
      "metadata": {
        "id": "KAc4tsH8CvCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types of Perceptrons:**\n",
        "\n",
        "### **1. Single-Layer Perceptron (Simple Perceptron):**\n",
        "\n",
        "**Structure:**\n",
        "- One input layer connected directly to one output layer\n",
        "\n",
        "- No hidden layers\n",
        "\n",
        "- Each output unit is an independent perceptron\n",
        "\n",
        "**Capabilities:**\n",
        "- Can only solve linearly separable problems\n",
        "\n",
        "- **Examples:** `AND`, `OR`, `NOT` gates\n",
        "\n",
        "- Cannot solve `XOR` problem\n",
        "\n",
        "**Mathematical limitation:**\n",
        "Can only learn decision boundaries that are hyperplanes in the input space.\n",
        "\n",
        "### 2. Multi-Layer Perceptron (MLP)\n",
        "\n",
        "**Structure:**\n",
        "- Input layer, one or more hidden layers, and output layer\n",
        "\n",
        "- Each layer fully connected to the next\n",
        "\n",
        "- Uses non-linear activation functions\n",
        "\n",
        "**Capabilities:**\n",
        "- Can solve non-linearly separable problems\n",
        "\n",
        "- Universal function approximator (with sufficient hidden units)\n",
        "\n",
        "- Can learn complex decision boundaries\n",
        "\n",
        "**Key differences from single-layer:**\n",
        "- Requires backpropagation for training\n",
        "\n",
        "- Uses continuous activation functions (`sigmoid`, `tanh`, `ReLU`)\n",
        "\n",
        "- Can have multiple outputs\n",
        "\n",
        "### 3. Voted Perceptron\n",
        "\n",
        "**Concept:**\n",
        "- Maintains multiple perceptrons from different stages of training\n",
        "\n",
        "- Final decision based on weighted voting\n",
        "\n",
        "- Each perceptron gets a vote proportional to how long it survived\n",
        "\n",
        "**Advantages:**\n",
        "- Better generalization than single perceptron\n",
        "\n",
        "- Handles noisy data more effectively\n",
        "\n",
        "### **4. Averaged Perceptron:**\n",
        "\n",
        "**Concept:**\n",
        "- Maintains running average of all weight vectors during training\n",
        "\n",
        "- Final weights are the average of all intermediate weights\n",
        "\n",
        "- Reduces overfitting to training data\n",
        "\n",
        "**Benefits:**\n",
        "- More stable than standard perceptron\n",
        "\n",
        "- Better performance on test data\n",
        "\n",
        "- Computationally efficient\n",
        "\n",
        "## **Detailed Learning Process:**\n",
        "\n",
        "### Training Phase:\n",
        "1. **`Initialize weights`**: Usually to small random values\n",
        "\n",
        "2. **`Present training examples`**: One at a time or in batches\n",
        "\n",
        "3. **`Calculate output`**: Using current weights\n",
        "\n",
        "4. **`Compare with target`**: Calculate error\n",
        "\n",
        "5. **`Update weights`**: Based on learning rule\n",
        "\n",
        "6. **`Repeat`**: Until convergence or maximum iterations\n",
        "\n",
        "### **Convergence Properties:**\n",
        "- **`Perceptron Convergence Theorem`**: For linearly separable data, the perceptron learning algorithm will converge in finite steps\n",
        "\n",
        "- **`Non-separable case`**: Algorithm may oscillate indefinitely\n",
        "\n",
        "- **`Learning rate impact`**: Too high causes instability, too low causes slow convergence\n",
        "\n",
        "## **Limitations and Solutions:**\n",
        "\n",
        "### **Major Limitations:**\n",
        "\n",
        "1. **`Linear separability`**: Cannot solve XOR and other non-linearly separable problems\n",
        "\n",
        "2. **`Binary output`**: Limited to classification tasks\n",
        "\n",
        "3. **`No probabilistic interpretation`**: Cannot provide confidence measures\n",
        "\n",
        "### **Historical Solutions:**\n",
        "1. **`Multi-layer networks`**: Overcome linear separability limitation\n",
        "\n",
        "2. **`Continuous activation functions`**: Enable gradient-based learning\n",
        "\n",
        "3. **`Ensemble methods`**: Combine multiple perceptrons for better performance\n",
        "\n",
        "## **Modern Relevance:**\n",
        "\n",
        "While basic perceptrons are rarely used alone in modern applications, their principles remain fundamental:\n",
        "\n",
        "- **`Feature engineering`**: Understanding linear separability helps in feature design\n",
        "\n",
        "- **`Ensemble methods`**: Perceptrons serve as weak learners in ensemble algorithms\n",
        "\n",
        "- **`Online learning`**: Perceptron-style updates are used in streaming data scenarios\n",
        "\n",
        "- **`Large-scale learning`**: Simple perceptrons are computationally efficient for big data\n",
        "\n",
        "The perceptron's elegance lies in its simplicity and mathematical tractability, making it an excellent educational tool for understanding the foundations of machine learning and neural networks. Despite its limitations, the concepts it introduced—`weighted inputs`, `threshold activation`, and `iterative learning`—remain central to all modern neural network architectures."
      ],
      "metadata": {
        "id": "vbVjglE5QWQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "u77XpsCl2bbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Single Perceptron vs Human Neuron:**\n",
        "\n",
        "**Single Perceptron:**\n",
        "- Sigle Perceptron the most basic artificial unit that mimics aspects of biological neurons\n",
        "\n",
        "- However, it's a **`highly simplified`** model of a human neuron\n",
        "\n",
        "- Real biological neurons are vastly more complex with:\n",
        "  - Thousands of synaptic connections\n",
        "  - Complex temporal dynamics and spike patterns\n",
        "  - Chemical neurotransmitter systems\n",
        "  - Non-linear dendritic processing\n",
        "  - Plasticity mechanisms far more sophisticated than simple weight updates\n",
        "\n",
        "**`So while a single perceptron is inspired by neurons, it's more accurate to say it captures only the basic concept of \"weighted inputs → threshold → output\" rather than being equivalent to a human brain neuron.`**\n",
        "\n",
        "## **Architecture Terminology Clarification:**\n",
        "\n",
        "### **Single Perceptron Architecture:**  \n",
        "- **`One perceptron = One artificial neuron`**\n",
        "- This is indeed the most basic form\n",
        "- Can only solve linearly separable problems\n",
        "\n",
        "### **Multiple Perceptrons - Two Scenarios:**\n",
        "\n",
        "**1. Multiple Independent Perceptrons (Parallel):**\n",
        "\n",
        "```\n",
        "Input → [Perceptron 1] → Output 1\n",
        "Input → [Perceptron 2] → Output 2  \n",
        "Input → [Perceptron 3] → Output 3\n",
        "```\n",
        "\n",
        "- Still considered single-layer architecture\n",
        "- Each perceptron solves a separate binary classification\n",
        "- Used for multi-class problems (one-vs-all approach)\n",
        "\n",
        "**2. Multi-Layer Perceptron (Sequential Layers):**\n",
        "\n",
        "```\n",
        "Input → [Hidden Layer Perceptrons] → [Output Layer Perceptrons] → Output\n",
        "```\n",
        "\n",
        "- This creates a **neural network**\n",
        "- Perceptrons are organized in layers\n",
        "- Information flows through multiple processing stages"
      ],
      "metadata": {
        "id": "IsdlDMEV_qh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron vs Neural Network:**\n",
        "\n",
        "> **`Multi-Layer Perceptron (MLP) IS a Neural Network`**\n",
        "\n",
        "**MLP characteristics:**\n",
        "- Multiple layers of perceptrons/neurons\n",
        "\n",
        "- Fully connected between adjacent layers\n",
        "\n",
        "- Uses non-linear activation functions (not just step functions)\n",
        "\n",
        "- Trained with backpropagation\n",
        "\n",
        "**Why it's called both:**\n",
        "- **Historically**: Called \"Multi-Layer Perceptron\" because it evolved from single perceptrons\n",
        "\n",
        "- **Functionally**: It **is** a neural network - specifically a type of feedforward neural network\n",
        "\n",
        "- **Modern usage**: People often use \"neural network\" and \"MLP\" interchangeably\n",
        "\n",
        "**The Relationship:**\n",
        "\n",
        "```\n",
        "Neural Networks (Broad Category)\n",
        "├── Feedforward Neural Networks\n",
        "│   ├── Multi-Layer Perceptron (MLP)\n",
        "│   └── Convolutional Neural Networks (CNN)\n",
        "├── Recurrent Neural Networks (RNN)\n",
        "├── Transformer Networks\n",
        "└── Other architectures...\n",
        "```"
      ],
      "metadata": {
        "id": "oY9Xe2QLVxZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Differences in Detail:**\n",
        "\n",
        "### **Single Perceptron:**\n",
        "- **`Layers`**: Input → Output (no hidden layers)\n",
        "\n",
        "- **`Capability`**: Linear decision boundaries only\n",
        "\n",
        "- **`Problems it can solve`**: `AND`, `OR` gates\n",
        "\n",
        "- **`Problems it cannot solve`**: `XOR`, complex patterns\n",
        "\n",
        "### **Multi-Layer Perceptron (Neural Network):**\n",
        "- **Layers**: Input → Hidden Layer(s) → Output\n",
        "- **Capability**: Non-linear decision boundaries\n",
        "- **Universal approximation**: Can theoretically approximate any continuous function\n",
        "- **Problems it can solve**: XOR, image recognition, complex pattern matching\n",
        "\n",
        "## **Modern Terminology:**\n",
        "\n",
        "**What we call \"Neural Networks\" today typically refers to:**\n",
        "\n",
        "- Multi-layer architectures (MLPs or more complex)\n",
        "\n",
        "- Use of non-linear activation functions\n",
        "\n",
        "- Gradient-based training (backpropagation)\n",
        "\n",
        "- Multiple neurons per layer\n",
        "\n",
        "**`Single perceptrons are rarely called \"neural networks\" in modern usage`** - they're considered the building blocks or historical predecessors.\n",
        "\n",
        "## **Practical Example:**\n",
        "\n",
        "**Single Perceptron solving `AND gate`:**\n",
        "```\n",
        "x1=0, x2=0 → output=0 ✓\n",
        "x1=0, x2=1 → output=0 ✓  \n",
        "x1=1, x2=0 → output=0 ✓\n",
        "x1=1, x2=1 → output=1 ✓\n",
        "```\n",
        "\n",
        "**Single Perceptron trying `XOR gate`:**\n",
        "```\n",
        "x1=0, x2=0 → output=0 ✓\n",
        "x1=0, x2=1 → output=1 ✓\n",
        "x1=1, x2=0 → output=1 ✓\n",
        "x1=1, x2=1 → output=0 ✗ (Cannot achieve this with linear boundary)\n",
        "```\n",
        "\n",
        "**MLP solving `XOR gate`:**\n",
        "- Requires at least one hidden layer with 2 neurons\n",
        "\n",
        "- Can create the necessary non-linear decision boundary\n",
        "\n",
        "- Successfully solves the problem\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "- **Single perceptron**: Most basic artificial neuron, inspired by but much simpler than biological neurons\n",
        "\n",
        "- **Multiple perceptrons in layers**: Creates a Multi-Layer Perceptron, which **is** a neural network\n",
        "\n",
        "- **MLP = Neural Network**: These terms are essentially equivalent in most contexts\n",
        "\n",
        "- **The key transition**: From linear (single perceptron) to non-linear capabilities (MLP/neural network)"
      ],
      "metadata": {
        "id": "G-7lCY2PaxvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "---\n",
        "----\n",
        "----\n",
        "----\n",
        "-----\n",
        "----\n",
        "----\n",
        "----\n",
        "----"
      ],
      "metadata": {
        "id": "_yaugn0h_qel"
      }
    }
  ]
}