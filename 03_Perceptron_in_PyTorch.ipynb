{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh21csCes4b2lni3Wn+B0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajesh-coventry/Foundational-Neural-Network-Perceptron-PyTorch/blob/master/03_Perceptron_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perceptron:**"
      ],
      "metadata": {
        "id": "7WBHJcW4SQck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is a Perceptron?**\n",
        "\n",
        "A **`perceptron`** is the simplest form of an artificial neural network and serves as the fundamental building block of deep learning.\n",
        "\n",
        "Think of it as a mathematical model that mimics how a single neuron in the brain works.\n",
        "\n",
        "### **How a Perceptron Works:**\n",
        "1. **Inputs**: It receives multiple input values (like features of your data)\n",
        "\n",
        "2. **Weights**: Each input has an associated weight that determines its importance\n",
        "\n",
        "3. **Weighted Sum**: It calculates the weighted sum of all inputs\n",
        "\n",
        "4. **Bias**: Adds a bias term (like an intercept in linear regression)\n",
        "\n",
        "5. **Activation Function**: Applies an activation function to produce the final output\n",
        "\n",
        "The mathematical formula is:\n",
        "\n",
        "> **output = activation_function(w₁x₁ + w₂x₂ + ... + wₙxₙ + bias)**\n",
        "\n",
        "## **Data Types in Perceptrons:**\n",
        "\n",
        "**Perceptrons work with:**\n",
        "\n",
        "- **`Input Data`**: Numerical features (integers, floats)\n",
        "\n",
        "- **`Weights`**: Float values that get learned during training\n",
        "\n",
        "- **`Bias`**: A single float value\n",
        "\n",
        "- **`Output`**:\n",
        "  - `Binary` (0 or 1) for classification\n",
        "  - `Continuous` values for regression\n",
        "\n",
        "## **Types of Problems Perceptrons Solve:**\n",
        "\n",
        "1. **Binary Classification**: Distinguishing between two classes (spam vs. not spam)\n",
        "\n",
        "2. **Linear Regression**: Predicting continuous values\n",
        "\n",
        "3. **Linearly Separable Problems**: Problems where data can be separated by a straight line\n",
        "\n",
        "**Limitations**: Single perceptrons can only solve linearly separable problems (like AND, OR logic gates) but cannot solve non-linearly separable problems (like XOR).\n",
        "\n",
        "Now, let me create a complete PyTorch implementation:"
      ],
      "metadata": {
        "id": "-MxLXDkDSBBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: CREATE A CUSTOM DATASET\n",
        "# =============================================================================\n",
        "\n",
        "def create_dataset():\n",
        "    \"\"\"\n",
        "    Create a synthetic binary classification dataset\n",
        "    This simulates a real-world scenario where we need to classify data points\n",
        "    \"\"\"\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "\n",
        "    # Generate a 2D dataset with 1000 samples\n",
        "    # n_features=2: We'll use 2 features (x1, x2) for easy visualization\n",
        "    # n_redundant=0: No redundant features\n",
        "    # n_informative=2: Both features are informative\n",
        "    # n_clusters_per_class=1: One cluster per class\n",
        "    X, y = make_classification(\n",
        "        n_samples=1000,           # Total number of data points\n",
        "        n_features=2,             # Number of input features\n",
        "        n_redundant=0,            # No redundant features\n",
        "        n_informative=2,          # Both features contribute to classification\n",
        "        n_clusters_per_class=1,   # One cluster per class\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset created: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    print(f\"Class distribution: Class 0: {np.sum(y==0)}, Class 1: {np.sum(y==1)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: DEFINE THE PERCEPTRON MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Layer Perceptron implemented using PyTorch\n",
        "\n",
        "    This is the simplest neural network with:\n",
        "    - One linear layer (fully connected layer)\n",
        "    - One activation function (sigmoid for binary classification)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Initialize the perceptron\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "\n",
        "        # Linear layer: y = wx + b\n",
        "        # input_size: number of input features\n",
        "        # 1: number of output neurons (binary classification)\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "        # Sigmoid activation function for binary classification\n",
        "        # Sigmoid maps any real number to a value between 0 and 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        print(f\"Perceptron initialized with {input_size} input features\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output probabilities of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Step 1: Apply linear transformation (wx + b)\n",
        "        linear_output = self.linear(x)\n",
        "\n",
        "        # Step 2: Apply sigmoid activation function\n",
        "        output = self.sigmoid(linear_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_weights_and_bias(self):\n",
        "        \"\"\"\n",
        "        Get the learned weights and bias for analysis\n",
        "\n",
        "        Returns:\n",
        "            tuple: (weights, bias)\n",
        "        \"\"\"\n",
        "        weights = self.linear.weight.data.numpy()\n",
        "        bias = self.linear.bias.data.numpy()\n",
        "        return weights, bias\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: DATA PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_data(X, y):\n",
        "    \"\"\"\n",
        "    Preprocess the data for training\n",
        "\n",
        "    Args:\n",
        "        X (np.array): Input features\n",
        "        y (np.array): Target labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: Processed training and testing sets\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    # 80% for training, 20% for testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Standardize the features (mean=0, std=1)\n",
        "    # This helps the model converge faster and perform better\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)  # Use same scaling as training\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)  # Add dimension for compatibility\n",
        "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "    print(f\"Training set: {X_train_tensor.shape[0]} samples\")\n",
        "    print(f\"Testing set: {X_test_tensor.shape[0]} samples\")\n",
        "\n",
        "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, scaler\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: TRAINING FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def train_perceptron(model, X_train, y_train, num_epochs=1000, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Train the perceptron model\n",
        "\n",
        "    Args:\n",
        "        model: The perceptron model to train\n",
        "        X_train: Training input features\n",
        "        y_train: Training target labels\n",
        "        num_epochs: Number of training iterations\n",
        "        learning_rate: Learning rate for optimization\n",
        "\n",
        "    Returns:\n",
        "        list: Training loss history\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs with learning rate {learning_rate}\")\n",
        "\n",
        "    # Define loss function (Binary Cross Entropy for binary classification)\n",
        "    # This measures how far our predictions are from the true labels\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Define optimizer (Stochastic Gradient Descent)\n",
        "    # This updates the model weights to minimize the loss\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Store training history for analysis\n",
        "    train_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Zero the gradients (PyTorch accumulates gradients by default)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        outputs = model(X_train)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss for plotting\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Print progress every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return train_losses\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: TESTING FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def test_perceptron(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Test the trained perceptron model\n",
        "\n",
        "    Args:\n",
        "        model: Trained perceptron model\n",
        "        X_test: Testing input features\n",
        "        y_test: Testing target labels\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predictions, probabilities, accuracy)\n",
        "    \"\"\"\n",
        "    print(\"Testing the model...\")\n",
        "\n",
        "    # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation for efficiency\n",
        "    with torch.no_grad():\n",
        "        # Get probability predictions\n",
        "        test_outputs = model(X_test)\n",
        "\n",
        "        # Convert probabilities to binary predictions\n",
        "        # If probability > 0.5, predict class 1, otherwise class 0\n",
        "        predicted = (test_outputs > 0.5).float()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = (predicted == y_test).float()\n",
        "        accuracy = correct.mean()\n",
        "\n",
        "        print(f'Test Accuracy: {accuracy.item():.4f} ({accuracy.item()*100:.2f}%)')\n",
        "\n",
        "        return predicted.numpy(), test_outputs.numpy(), accuracy.item()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: ANALYSIS AND VISUALIZATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_predictions(model, X_test, y_test, predicted, probabilities):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the model's predictions\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        y_test: True test labels\n",
        "        predicted: Model predictions\n",
        "        probabilities: Prediction probabilities\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PREDICTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Convert tensors to numpy for analysis\n",
        "    X_test_np = X_test.numpy()\n",
        "    y_test_np = y_test.numpy().flatten()\n",
        "    predicted_np = predicted.flatten()\n",
        "    probabilities_np = probabilities.flatten()\n",
        "\n",
        "    # 1. Confusion Matrix Analysis\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "    cm = confusion_matrix(y_test_np, predicted_np)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_np, predicted_np))\n",
        "\n",
        "    # 2. Analyze model weights and bias\n",
        "    weights, bias = model.get_weights_and_bias()\n",
        "    print(f\"\\nLearned Weights: {weights}\")\n",
        "    print(f\"Learned Bias: {bias}\")\n",
        "\n",
        "    # 3. Decision boundary equation\n",
        "    print(f\"\\nDecision Boundary Equation:\")\n",
        "    print(f\"w1*x1 + w2*x2 + bias = 0\")\n",
        "    print(f\"{weights[0][0]:.4f}*x1 + {weights[0][1]:.4f}*x2 + {bias[0]:.4f} = 0\")\n",
        "\n",
        "    # 4. Show some example predictions\n",
        "    print(f\"\\nSample Predictions:\")\n",
        "    print(\"Index | Feature1 | Feature2 | True Label | Predicted | Probability\")\n",
        "    print(\"-\" * 65)\n",
        "    for i in range(min(10, len(X_test_np))):\n",
        "        print(f\"{i:5d} | {X_test_np[i][0]:8.3f} | {X_test_np[i][1]:8.3f} | \"\n",
        "              f\"{int(y_test_np[i]):10d} | {int(predicted_np[i]):9d} | {probabilities_np[i]:11.3f}\")\n",
        "\n",
        "def plot_results(train_losses, X_test, y_test, predicted, model):\n",
        "    \"\"\"\n",
        "    Create visualizations of the training process and results\n",
        "\n",
        "    Args:\n",
        "        train_losses: Training loss history\n",
        "        X_test: Test features\n",
        "        y_test: True test labels\n",
        "        predicted: Model predictions\n",
        "        model: Trained model\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Training Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses)\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Data Distribution\n",
        "    plt.subplot(1, 3, 2)\n",
        "    X_test_np = X_test.numpy()\n",
        "    y_test_np = y_test.numpy().flatten()\n",
        "\n",
        "    # Plot true labels\n",
        "    colors = ['red', 'blue']\n",
        "    labels = ['Class 0', 'Class 1']\n",
        "    for i in range(2):\n",
        "        mask = y_test_np == i\n",
        "        plt.scatter(X_test_np[mask, 0], X_test_np[mask, 1],\n",
        "                   c=colors[i], label=labels[i], alpha=0.7)\n",
        "\n",
        "    plt.title('True Data Distribution')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: Predictions vs True Labels\n",
        "    plt.subplot(1, 3, 3)\n",
        "    predicted_np = predicted.flatten()\n",
        "\n",
        "    # Plot predictions\n",
        "    for i in range(2):\n",
        "        mask = predicted_np == i\n",
        "        plt.scatter(X_test_np[mask, 0], X_test_np[mask, 1],\n",
        "                   c=colors[i], label=f'Predicted {labels[i]}',\n",
        "                   alpha=0.7, marker='x')\n",
        "\n",
        "    plt.title('Model Predictions')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function that orchestrates the entire perceptron training and testing process\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"SINGLE LAYER PERCEPTRON - COMPLETE IMPLEMENTATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Create dataset\n",
        "    X, y = create_dataset()\n",
        "\n",
        "    # Step 2: Preprocess data\n",
        "    X_train, X_test, y_train, y_test, scaler = preprocess_data(X, y)\n",
        "\n",
        "    # Step 3: Initialize model\n",
        "    input_size = X_train.shape[1]  # Number of features\n",
        "    model = Perceptron(input_size)\n",
        "\n",
        "    # Display model architecture\n",
        "    print(f\"\\nModel Architecture:\")\n",
        "    print(f\"Input size: {input_size}\")\n",
        "    print(f\"Output size: 1 (binary classification)\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Step 4: Train the model\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TRAINING PHASE\")\n",
        "    print('='*50)\n",
        "    train_losses = train_perceptron(model, X_train, y_train,\n",
        "                                  num_epochs=1000, learning_rate=0.1)\n",
        "\n",
        "    # Step 5: Test the model\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TESTING PHASE\")\n",
        "    print('='*50)\n",
        "    predicted, probabilities, accuracy = test_perceptron(model, X_test, y_test)\n",
        "\n",
        "    # Step 6: Analyze results\n",
        "    analyze_predictions(model, X_test, y_test, predicted, probabilities)\n",
        "\n",
        "    # Step 7: Create visualizations\n",
        "    plot_results(train_losses, X_test, y_test, predicted, model)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYSIS COMPLETE!\")\n",
        "    print(f\"Final Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Run the complete implementation\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2wizcyamSKRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Key Concepts Explained in the Code:\n",
        "\n",
        "### 1. **Perceptron Architecture**\n",
        "- **Input Layer**: Receives 2 features (for our 2D dataset)\n",
        "- **Linear Layer**: Performs weighted sum (wx + b)\n",
        "- **Activation Function**: Sigmoid function converts output to probability\n",
        "\n",
        "### 2. **Training Process**\n",
        "- **Forward Pass**: Input → Linear → Sigmoid → Output\n",
        "- **Loss Calculation**: Binary Cross Entropy measures prediction error\n",
        "- **Backward Pass**: Calculate gradients using backpropagation\n",
        "- **Weight Update**: Adjust weights using gradient descent\n",
        "\n",
        "### 3. **Key Components**\n",
        "- **Weights**: Learned parameters that determine feature importance\n",
        "- **Bias**: Learned offset term\n",
        "- **Learning Rate**: Controls how fast the model learns\n",
        "- **Epochs**: Number of complete passes through the data\n",
        "\n",
        "### 4. **Data Preprocessing**\n",
        "- **Standardization**: Scales features to have mean=0, std=1\n",
        "- **Train/Test Split**: Separates data for training and evaluation\n",
        "- **Tensor Conversion**: Converts NumPy arrays to PyTorch tensors\n",
        "\n",
        "### 5. **Analysis Features**\n",
        "- **Confusion Matrix**: Shows correct vs incorrect predictions\n",
        "- **Decision Boundary**: Mathematical equation separating classes\n",
        "- **Training Loss**: Shows how error decreases over time\n",
        "- **Accuracy**: Percentage of correct predictions\n",
        "\n",
        "## What the Code Does:\n",
        "\n",
        "1. **Creates a synthetic dataset** with 1000 samples and 2 features\n",
        "2. **Implements a perceptron** with linear layer + sigmoid activation\n",
        "3. **Trains the model** using gradient descent for 1000 epochs\n",
        "4. **Tests the model** on unseen data and calculates accuracy\n",
        "5. **Analyzes predictions** with detailed metrics and visualizations\n",
        "6. **Shows the decision boundary** equation learned by the model\n",
        "\n",
        "## Expected Output:\n",
        "- Training accuracy: ~85-95%\n",
        "- Test accuracy: Similar to training (good generalization)\n",
        "- Visualizations showing data distribution and predictions\n",
        "- Decision boundary equation that separates the two classes\n",
        "\n",
        "This implementation demonstrates all fundamental concepts of perceptrons and provides a solid foundation for understanding more complex neural networks!"
      ],
      "metadata": {
        "id": "tKuXlLXySI5U"
      }
    }
  ]
}